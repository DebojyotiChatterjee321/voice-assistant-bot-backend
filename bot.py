#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""voice_assistant_bot - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Elevenlabs (Speech-to-Text)
- Google_Gemini (LLM)
- Elevenlabs (Text-to-Speech)

Run the bot using::

    python bot.py
"""

import asyncio
import os
import re
import sys
import time
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Dict, Optional

from dotenv import load_dotenv
from loguru import logger

from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    CancelFrame,
    EndFrame,
    LLMFullResponseEndFrame,
    LLMFullResponseStartFrame,
    LLMRunFrame,
    LLMTextFrame,
    StartFrame,
    TranscriptionFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

from db_tools import DatabaseTools

load_dotenv(override=True)

@dataclass
class TurnTiming:
    stt_ms: float = 0.0
    dialogue_ms: float = 0.0
    tts_ms: float = 0.0


class TimingObserver(FrameProcessor):
    """Collect per-turn timing for STT, scripted dialogue, and TTS."""

    def __init__(self) -> None:
        super().__init__(name="TimingObserver", enable_direct_mode=True)
        self._stt_start: Optional[float] = None
        self._dialogue_start: Optional[float] = None
        self._tts_start: Optional[float] = None
        self._turn_id = 0
        self._timings: Dict[int, TurnTiming] = {}
        self._current_text = ""

    def _current(self) -> TurnTiming:
        timing = self._timings.setdefault(self._turn_id, TurnTiming())
        return timing

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StartFrame):
            await self.push_frame(frame, direction)
            return

        if isinstance(frame, TranscriptionFrame) and direction == FrameDirection.DOWNSTREAM:
            self._turn_id += 1
            processing_time = frame.metadata.get("processing_time")
            if processing_time:
                self._current().stt_ms = processing_time * 1000
            self._dialogue_start = time.perf_counter()
            self._current()
        elif isinstance(frame, LLMFullResponseStartFrame):
            if self._dialogue_start:
                self._current().dialogue_ms = (time.perf_counter() - self._dialogue_start) * 1000
            self._dialogue_start = None
            self._tts_start = time.perf_counter()
        elif isinstance(frame, LLMFullResponseEndFrame):
            if self._tts_start:
                self._current().tts_ms += (time.perf_counter() - self._tts_start) * 1000
            self._tts_start = None
            timing = self._timings.get(self._turn_id)
            if timing:
                total_ms = timing.stt_ms + timing.dialogue_ms + timing.tts_ms
                logger.info(
                    f"Turn {self._turn_id}: '{self._current_text}' — STT: {timing.stt_ms:.1f} ms | Dialogue: {timing.dialogue_ms:.1f} ms | TTS: {timing.tts_ms:.1f} ms | Total: {total_ms:.1f} ms"
                )
        elif isinstance(frame, LLMTextFrame):
            self._current_text = frame.text or ""

        await self.push_frame(frame, direction)


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    # Speech-to-Text service (streaming)
    stt = DeepgramSTTService(
        api_key=os.getenv("DEEPGRAM_API_KEY"),
    )

    # Text-to-Speech service
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY2"),
        voice_id=os.getenv("ELEVENLABS_VOICE_ID"),
        aggregate_sentences=False,
    )

    system_prompt = (
        "You are a helpful e-commerce voice assistant. Respond naturally, conversationally, politely. Your responses should be concise and to the point, yet professional and informative."
        "Greet the with 'Hi, I'm June, am I speaking to Sarah?' Whether they say yes or no, ask for their email address."
        "You have access to database tools that can look up orders and products. Use these tools only when the user explicitly asks about an order or a product, and request identifying details first when needed. If the user asks about their orders, please ask them to give their email address or customer ID, if they haven't shared it already.For unrelated questions, answer without using the tools and gently guide the user back to shopping topics when appropriate."
    )

    messages = [
        {
            "role": "system",
            "content": system_prompt,
        }
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # LLM service
    llm = GoogleLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        model=os.getenv("GOOGLE_MODEL", "gemini-1.5-flash-latest"),
        system_instruction=system_prompt,
    )

    data_dir = Path(__file__).parent / "data"
    db_path_env = os.getenv("ORDERS_DB_PATH")
    db_path = Path(db_path_env) if db_path_env else Path(__file__).parent / "orders.db"

    db_tools = DatabaseTools(db_path=db_path, data_dir=data_dir)
    tool_functions = list(db_tools.tool_functions)
    for tool in tool_functions:
        llm.register_direct_function(tool)

    context.set_tools(ToolsSchema(standard_tools=tool_functions))
    context.set_tool_choice({"type": "auto"})

    rtvi = RTVIProcessor()
    timing_observer = TimingObserver()

    # Pipeline - assembled from reusable components
    pipeline = Pipeline([
        transport.input(),
        rtvi,
        stt,
        context_aggregator.user(),
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
        timing_observer,
    ])

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    # Render deployment configuration
    # Render sets PORT env var (default 10000) and requires binding to 0.0.0.0
    port = int(os.getenv("PORT", 10000))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"Configuring for Render deployment: host={host}, port={port}")
    
    # Configure command line arguments for Pipecat runner
    # Remove any existing port/host arguments to avoid conflicts
    args_to_remove = []
    for i, arg in enumerate(sys.argv):
        if arg in ["--port", "-p", "--host", "-h"]:
            args_to_remove.extend([i, i + 1])
        elif arg.startswith("--port=") or arg.startswith("--host="):
            args_to_remove.append(i)
    
    # Remove in reverse order to maintain indices
    for i in sorted(args_to_remove, reverse=True):
        if i < len(sys.argv):
            sys.argv.pop(i)
    
    # Add Render-specific configuration
    sys.argv.extend(["--host", host, "--port", str(port)])
    
    logger.info(f"Starting Pipecat with args: {sys.argv[1:]}")
    
    # Run Pipecat
    from pipecat.runner.run import main
    main()