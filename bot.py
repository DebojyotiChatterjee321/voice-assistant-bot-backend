@dataclass
class TurnTiming:
    stt_ms: float = 0.0
    dialogue_ms: float = 0.0
    tts_ms: float = 0.0


class TimingObserver(FrameProcessor):
    """Collect per-turn timing for STT, scripted dialogue, and TTS."""

    def __init__(self) -> None:
        super().__init__(name="TimingObserver", enable_direct_mode=True)
        self._stt_start: Optional[float] = None
        self._dialogue_start: Optional[float] = None
        self._tts_start: Optional[float] = None
        self._turn_id = 0
        self._timings: Dict[int, TurnTiming] = {}

    def _current(self) -> TurnTiming:
        timing = self._timings.setdefault(self._turn_id, TurnTiming())
        return timing

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StartFrame):
            await self.push_frame(frame, direction)
            return

        if isinstance(frame, TranscriptionFrame) and direction == FrameDirection.DOWNSTREAM:
            self._turn_id += 1
            self._stt_start = frame.metadata.get("stt_start")
            stt_end = frame.metadata.get("stt_end")
            if self._stt_start and stt_end:
                self._current().stt_ms = (stt_end - self._stt_start) * 1000
            self._dialogue_start = time.perf_counter()
            self._current()
        elif isinstance(frame, LLMFullResponseStartFrame):
            if self._dialogue_start:
                self._current().dialogue_ms = (time.perf_counter() - self._dialogue_start) * 1000
            self._dialogue_start = None
            self._tts_start = time.perf_counter()
        elif isinstance(frame, LLMFullResponseEndFrame):
            if self._tts_start:
                self._current().tts_ms += (time.perf_counter() - self._tts_start) * 1000
            self._tts_start = None
            timing = self._timings.get(self._turn_id)
            if timing:
                logger.info(
                    "Turn %s timings — STT: %.1f ms | Fixed Dialogue: %.1f ms | TTS: %.1f ms",
                    self._turn_id,
                    timing.stt_ms,
                    timing.dialogue_ms,
                    timing.tts_ms,
                )
        elif frame.__class__.__name__ == "TTSAudioRawFrame":
            if not self._tts_start:
                self._tts_start = time.perf_counter()

        await self.push_frame(frame, direction)
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""voice_assistant_bot - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Elevenlabs (Speech-to-Text)
- Google_Gemini (LLM)
- Elevenlabs (Text-to-Speech)

Run the bot using::

    python bot.py
"""

import asyncio
import os
import re
import sys
import time
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Dict, Optional

from aiohttp import ClientSession
from dotenv import load_dotenv
from loguru import logger

from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    CancelFrame,
    EndFrame,
    LLMFullResponseEndFrame,
    LLMFullResponseStartFrame,
    LLMTextFrame,
    StartFrame,
    TranscriptionFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.elevenlabs.stt import ElevenLabsSTTService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


class DialogueState(Enum):
    """State machine for the scripted conversation."""

    INIT = auto()
    AWAITING_ORDER_REQUEST = auto()
    AWAITING_THANK_YOU = auto()
    COMPLETE = auto()


def _normalize(text: str) -> str:
    """Lowercase and strip punctuation for robust matching."""

    return re.sub(r"[^a-z0-9 ]", "", text.lower()).strip()


class ScriptedDialogueProcessor(FrameProcessor):
    """Frame processor that enforces a deterministic conversation."""

    GREETING = "Hello, how can I help you today?"
    ORDER_RESPONSE = "Your order is shipped yesterday and will be delivered tomorrow"
    ORDER_EXPECTED = _normalize("I need details on my order")
    THANK_YOU_EXPECTED = _normalize("Thank you")

    def __init__(self) -> None:
        super().__init__(name="ScriptedDialogueProcessor", enable_direct_mode=True)
        self._state = DialogueState.INIT
        self._ready_event: asyncio.Event = asyncio.Event()

    async def begin_dialogue(self) -> None:
        """Emit the scripted greeting once the pipeline is ready."""

        await self._ready_event.wait()
        if self._state != DialogueState.INIT:
            logger.debug("Scripted dialogue already started; skipping greeting.")
            return

        logger.info("Scripted dialogue: sending greeting")
        await self._emit_assistant(self.GREETING)
        self._state = DialogueState.AWAITING_ORDER_REQUEST

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StartFrame):
            await self.push_frame(frame, direction)
            self._ready_event.set()
            return

        if isinstance(frame, (EndFrame, CancelFrame)):
            await self.push_frame(frame, direction)
            self._ready_event.clear()
            return

        if isinstance(frame, TranscriptionFrame):
            await self._handle_transcription(frame)
            await self.push_frame(frame, direction)
            return

        await self.push_frame(frame, direction)

    async def _handle_transcription(self, frame: TranscriptionFrame) -> None:
        text = (frame.text or "").strip()
        if not text:
            return

        normalized = _normalize(text)
        logger.debug(f"Scripted dialogue received transcription: '{text}'")

        if self._state == DialogueState.AWAITING_ORDER_REQUEST:
            if normalized == self.ORDER_EXPECTED:
                logger.info("Scripted dialogue: received expected order request")
                await self._emit_assistant(self.ORDER_RESPONSE)
                self._state = DialogueState.AWAITING_THANK_YOU
            else:
                logger.warning(
                    "Unexpected user utterance while awaiting order request: '%s'",
                    text,
                )
                await self._emit_assistant(
                    "Please say 'I need details on my order."
                )
        elif self._state == DialogueState.AWAITING_THANK_YOU:
            if normalized == self.THANK_YOU_EXPECTED:
                logger.info("Scripted dialogue: conversation complete, awaiting disconnect")
                self._state = DialogueState.COMPLETE
            else:
                logger.warning(
                    "Unexpected user utterance while awaiting thank you: '%s'",
                    text,
                )
                await self._emit_assistant(
                    "Please respond with 'Thank you' to end the session."
                )
        else:
            logger.debug("Scripted dialogue ignoring transcription in state %s", self._state)

    async def _emit_assistant(self, text: str) -> None:
        await self.push_frame(LLMFullResponseStartFrame(), FrameDirection.DOWNSTREAM)
        await self.push_frame(LLMTextFrame(text=text), FrameDirection.DOWNSTREAM)
        await self.push_frame(LLMFullResponseEndFrame(), FrameDirection.DOWNSTREAM)


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    async with ClientSession() as session:
        # Speech-to-Text service
        stt = ElevenLabsSTTService(
            api_key=os.getenv("ELEVENLABS_API_KEY"),
            aiohttp_session=session
        )

        # Text-to-Speech service
        tts = ElevenLabsTTSService(
            api_key=os.getenv("ELEVENLABS_API_KEY"),
            voice_id=os.getenv("ELEVENLABS_VOICE_ID")
        )

        rtvi = RTVIProcessor()
        dialogue = ScriptedDialogueProcessor()
        timing_observer = TimingObserver()

        # Pipeline - assembled from reusable components
        pipeline = Pipeline([
            transport.input(),
            rtvi,
            stt,
            dialogue,
            tts,
            transport.output(),
            timing_observer,
        ])

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
            observers=[
                RTVIObserver(rtvi),
            ],
        )

        @transport.event_handler("on_client_connected")
        async def on_client_connected(transport, client):
            logger.info("Client connected")
            await dialogue.begin_dialogue()

        @transport.event_handler("on_client_disconnected")
        async def on_client_disconnected(transport, client):
            logger.info("Client disconnected")
            await task.cancel()

        runner = PipelineRunner(handle_sigint=False)
        await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    # Render deployment configuration
    # Render sets PORT env var (default 10000) and requires binding to 0.0.0.0
    port = int(os.getenv("PORT", 10000))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"Configuring for Render deployment: host={host}, port={port}")
    
    # Configure command line arguments for Pipecat runner
    # Remove any existing port/host arguments to avoid conflicts
    args_to_remove = []
    for i, arg in enumerate(sys.argv):
        if arg in ["--port", "-p", "--host", "-h"]:
            args_to_remove.extend([i, i + 1])
        elif arg.startswith("--port=") or arg.startswith("--host="):
            args_to_remove.append(i)
    
    # Remove in reverse order to maintain indices
    for i in sorted(args_to_remove, reverse=True):
        if i < len(sys.argv):
            sys.argv.pop(i)
    
    # Add Render-specific configuration
    sys.argv.extend(["--host", host, "--port", str(port)])
    
    logger.info(f"Starting Pipecat with args: {sys.argv[1:]}")
    
    # Run Pipecat
    from pipecat.runner.run import main
    main()