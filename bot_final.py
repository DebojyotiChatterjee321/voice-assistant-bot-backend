#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""voice_assistant_bot - Pipecat Voice Agent

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Elevenlabs (Speech-to-Text)
- Google_Gemini (LLM)
- Elevenlabs (Text-to-Speech)

Run the bot using::

    python bot.py
"""

import asyncio
import functools
import os
import re
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Any, Dict, List, Optional
from types import MethodType

from dotenv import load_dotenv
from loguru import logger

from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    CancelFrame,
    EndFrame,
    LLMContextFrame,
    LLMFullResponseEndFrame,
    LLMFullResponseStartFrame,
    LLMRunFrame,
    LLMTextFrame,
    StartFrame,
    TranscriptionFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

from db_tools import DatabaseTools
from deepgram.clients.live.v1 import LiveOptions

load_dotenv(override=True)

@dataclass
class TurnTiming:
    stt_ms: float = 0.0
    dialogue_ms: float = 0.0
    tts_ms: float = 0.0


class LLMContextPruner(FrameProcessor):
    """Trim LLM context history to reduce prompt size before inference."""

    def __init__(self, *, max_turns: int = 5, max_chars: int = 1000) -> None:
        super().__init__(name="LLMContextPruner", enable_direct_mode=True)
        self._max_turns = max(0, max_turns)
        self._max_chars = max(0, max_chars)
        self._max_messages = self._max_turns * 2 if self._max_turns else 0

    def _trim_text(self, value: str) -> str:
        if not self._max_chars or len(value) <= self._max_chars:
            return value
        return f"{value[: self._max_chars].rstrip()} …"

    def _trim_value(self, value: Any) -> Any:
        if isinstance(value, str):
            return self._trim_text(value)
        if isinstance(value, list):
            return [self._trim_value(item) for item in value]
        if isinstance(value, dict):
            return {key: self._trim_value(val) for key, val in value.items()}
        return value

    def _trim_message(self, message: dict) -> dict:
        trimmed = {**message}

        if "content" in trimmed:
            content = trimmed["content"]
            if isinstance(content, str):
                trimmed["content"] = self._trim_text(content)
            elif isinstance(content, list):
                trimmed["content"] = [self._trim_value(part) for part in content]

        if "parts" in trimmed and isinstance(trimmed["parts"], list):
            trimmed["parts"] = [self._trim_value(part) for part in trimmed["parts"]]

        return trimmed

    def _prune_messages(self, messages: List) -> List:
        if not messages:
            return messages

        system_messages: List = []
        rest = messages

        first = messages[0]
        if isinstance(first, dict) and first.get("role") == "system":
            system_messages = [first]
            rest = messages[1:]

        if self._max_messages and len(rest) > self._max_messages:
            drop = len(rest) - self._max_messages
            logger.debug("Pruning %s older LLM messages", drop)
            rest = rest[-self._max_messages :]

        trimmed_rest = [self._trim_message(msg) if isinstance(msg, dict) else msg for msg in rest]
        return system_messages + trimmed_rest

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, LLMContextFrame) and direction == FrameDirection.DOWNSTREAM:
            context = frame.context
            if context:
                messages = context.get_messages()
                pruned = self._prune_messages(messages)
                if pruned is not messages:
                    context.set_messages(pruned)

        await self.push_frame(frame, direction)


class TimingObserver(FrameProcessor):
    """Collect per-turn timing for STT, scripted dialogue, and TTS."""

    _QUEUE_META_KEY = "_timing_queue_enqueued"
    _TURN_META_KEY = "_timing_turn_id"

    def __init__(self) -> None:
        super().__init__(name="TimingObserver", enable_direct_mode=True)
        self._stt_start: Optional[float] = None
        self._dialogue_start: Optional[float] = None
        self._tts_start: Optional[float] = None
        self._turn_id = 0
        self._timings: Dict[int, TurnTiming] = {}
        self._current_text = ""
        self._component_order: List[str] = []
        self._turn_dwell: Dict[int, Dict[str, List[float]]] = {}
        self._instrumented: set[int] = set()

    def _current(self) -> TurnTiming:
        timing = self._timings.setdefault(self._turn_id, TurnTiming())
        return timing

    def attach_pipeline(self, pipeline: Pipeline) -> None:
        """Instrument all processors in the pipeline to capture queue dwell times."""

        for processor in pipeline.processors:
            if processor is self:
                continue
            self._instrument_processor(processor)

    def _instrument_processor(self, processor: FrameProcessor) -> None:
        proc_id = id(processor)
        if proc_id in self._instrumented:
            return

        proc_name = processor.name
        original_queue = processor.queue_frame

        async def queue_wrapper(proc, frame, direction=FrameDirection.DOWNSTREAM, callback=None):
            if direction == FrameDirection.DOWNSTREAM:
                queue_meta = frame.metadata.setdefault(self._QUEUE_META_KEY, {})
                queue_meta[proc_name] = time.perf_counter()
                if self._TURN_META_KEY not in frame.metadata:
                    frame.metadata[self._TURN_META_KEY] = self._turn_id
            return await original_queue(frame, direction, callback)

        async def before_process_handler(_, frame):
            await self._handle_before_process(proc_name, frame)

        processor.queue_frame = MethodType(queue_wrapper, processor)
        processor.add_event_handler("on_before_process_frame", before_process_handler)
        self._instrumented.add(proc_id)

    async def _handle_before_process(self, processor_name: str, frame) -> None:
        queue_meta = frame.metadata.get(self._QUEUE_META_KEY)
        if not queue_meta:
            return

        enqueue_ts = queue_meta.pop(processor_name, None)
        if enqueue_ts is None:
            return

        dwell_ms = (time.perf_counter() - enqueue_ts) * 1000
        turn_id = frame.metadata.get(self._TURN_META_KEY, self._turn_id)

        bucket = self._turn_dwell.get(turn_id)
        if bucket is None:
            bucket = defaultdict(list)
            self._turn_dwell[turn_id] = bucket

        bucket.setdefault(processor_name, []).append(dwell_ms)

        if processor_name not in self._component_order:
            self._component_order.append(processor_name)

    def _format_dwell_summary(self, turn_id: int) -> str:
        bucket = self._turn_dwell.pop(turn_id, None)
        if not bucket:
            return ""

        parts = []
        for processor_name in self._component_order:
            values = bucket.get(processor_name)
            if not values:
                continue
            avg_ms = sum(values) / len(values)
            parts.append(f"{processor_name}: {avg_ms:.1f}")

        return " | ".join(parts)

    async def process_frame(self, frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StartFrame):
            await self.push_frame(frame, direction)
            return

        if isinstance(frame, TranscriptionFrame) and direction == FrameDirection.DOWNSTREAM:
            self._turn_id += 1
            processing_time = frame.metadata.get("processing_time")
            if processing_time:
                self._current().stt_ms = processing_time * 1000
            self._dialogue_start = time.perf_counter()
            self._current()
            frame.metadata[self._TURN_META_KEY] = self._turn_id
            if self._turn_id not in self._turn_dwell:
                self._turn_dwell[self._turn_id] = defaultdict(list)
        elif isinstance(frame, LLMFullResponseStartFrame):
            if self._dialogue_start:
                self._current().dialogue_ms = (time.perf_counter() - self._dialogue_start) * 1000
            self._dialogue_start = None
            self._tts_start = time.perf_counter()
        elif isinstance(frame, LLMFullResponseEndFrame):
            if self._tts_start:
                self._current().tts_ms += (time.perf_counter() - self._tts_start) * 1000
            self._tts_start = None
            timing = self._timings.get(self._turn_id)
            if timing:
                total_ms = timing.stt_ms + timing.dialogue_ms + timing.tts_ms
                dwell_summary = self._format_dwell_summary(self._turn_id)
                logger.info(
                    f"Turn {self._turn_id}: '{self._current_text}' — STT: {timing.stt_ms:.1f} ms | Dialogue: {timing.dialogue_ms:.1f} ms | TTS: {timing.tts_ms:.1f} ms | Total: {total_ms:.1f} ms"
                    + (f" | Queue dwell(ms): {dwell_summary}" if dwell_summary else "")
                )
        elif isinstance(frame, LLMTextFrame):
            self._current_text = frame.text or ""

        await self.push_frame(frame, direction)


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    # Speech-to-Text service (streaming)
    stt = DeepgramSTTService(
        api_key=os.getenv("DEEPGRAM_API_KEY"),
        live_options=LiveOptions(
            encoding="linear16",
            language="en",
            model="nova-3-general",
            channels=1,
            interim_results=True,
            smart_format=True,
            punctuate=True,
            vad_events=True,
        ),
    )

    # Text-to-Speech service
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY2"),
        voice_id=os.getenv("ELEVENLABS_VOICE_ID"),
        aggregate_sentences=False,
    )

    system_prompt = (
        "You are a helpful e-commerce voice assistant. Respond naturally, conversationally, politely. Your responses should be concise and to the point, yet professional and informative."
        "Greet the with 'Hi User, I'm June, could you please share your email?'."
        "You have access to database tools that can look up orders and products. Use these tools only when the user explicitly asks about an order or a product, and request identifying details first when needed. If the user asks about their orders, please ask them to give their email address or customer ID, if they haven't shared it already.For unrelated questions, answer without using the tools and gently guide the user back to shopping topics when appropriate."
    )

    messages = [
        {
            "role": "system",
            "content": system_prompt,
        }
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)
    context_pruner = LLMContextPruner(max_turns=3, max_chars=600)

    # LLM service
    llm = GoogleLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        model=os.getenv("GOOGLE_MODEL", "gemini-1.5-flash-latest"),
        system_instruction=system_prompt,
        run_in_parallel=True,
    )

    data_dir = Path(__file__).parent / "data"
    db_path_env = os.getenv("ORDERS_DB_PATH")
    db_path = Path(db_path_env) if db_path_env else Path(__file__).parent / "orders.db"

    db_tools = DatabaseTools(db_path=db_path, data_dir=data_dir)
    original_tools = list(db_tools.tool_functions)

    def _wrap_tool(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                return await func(*args, **kwargs)
            except Exception as exc:  # pragma: no cover - defensive guard
                params = args[0] if args else kwargs.get("params")
                logger.exception("Tool %s failed: %s", getattr(params, "function_name", func.__name__), exc)
                if isinstance(params, FunctionCallParams):
                    await params.result_callback(
                        {
                            "status": "error",
                            "message": "Tool execution failed. Please try again.",
                        }
                    )
                return None

        return wrapper

    wrapped_tools = []
    for tool in original_tools:
        wrapped = _wrap_tool(tool)
        wrapped_tools.append(wrapped)
        llm.register_direct_function(wrapped)

    context.set_tools(ToolsSchema(standard_tools=wrapped_tools))
    context.set_tool_choice({"type": "auto"})

    rtvi = RTVIProcessor()
    timing_observer = TimingObserver()

    # Pipeline - assembled from reusable components
    pipeline = Pipeline([
        transport.input(),
        rtvi,
        stt,
        context_aggregator.user(),
        context_pruner,
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
        timing_observer,
    ])
    timing_observer.attach_pipeline(pipeline)

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    warm_lock = asyncio.Lock()
    warm_done = False

    async def warm_tts_connection() -> None:
        logger.info("Priming TTS connection")
        try:
            async for frame in tts.run_tts(" "):
                if frame is None:
                    break
        except Exception as exc:  # pragma: no cover - warmup best-effort
            logger.warning("TTS warmup failed: %s", exc)
        else:
            try:
                await tts.flush_audio()
            except Exception as exc:  # pragma: no cover - warmup best-effort
                logger.debug("TTS flush after warmup failed: %s", exc)
        finally:
            if hasattr(tts, "_started"):
                tts._started = False
            if hasattr(tts, "_context_id"):
                tts._context_id = None

    async def ensure_warm_paths():
        nonlocal warm_done
        if warm_done:
            return

        async with warm_lock:
            if warm_done:
                return

            logger.info("Priming LLM cache")

            warm_context = LLMContext(
                [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": "Warm-up ping"},
                ]
            )

            try:
                await llm.run_inference(warm_context)
            except Exception as exc:  # pragma: no cover - warmup best-effort
                logger.warning("LLM warmup failed: %s", exc)

            try:
                await warm_tts_connection()
            except Exception as exc:  # pragma: no cover - warmup best-effort
                logger.warning("TTS warmup attempt failed: %s", exc)

            warm_done = True

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        await ensure_warm_paths()
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    vad_analyzer=SileroVADAnalyzer(
                        params=VADParams(
                            confidence=0.75,
                            start_secs=0.12,
                            stop_secs=0.3,
                            min_volume=0.55,
                        )
                    ),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    # Render deployment configuration
    # Render sets PORT env var (default 10000) and requires binding to 0.0.0.0
    port = int(os.getenv("PORT", 10000))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"Configuring for Render deployment: host={host}, port={port}")
    
    # Configure command line arguments for Pipecat runner
    # Remove any existing port/host arguments to avoid conflicts
    args_to_remove = []
    for i, arg in enumerate(sys.argv):
        if arg in ["--port", "-p", "--host", "-h"]:
            args_to_remove.extend([i, i + 1])
        elif arg.startswith("--port=") or arg.startswith("--host="):
            args_to_remove.append(i)
    
    # Remove in reverse order to maintain indices
    for i in sorted(args_to_remove, reverse=True):
        if i < len(sys.argv):
            sys.argv.pop(i)
    
    # Add Render-specific configuration
    sys.argv.extend(["--host", host, "--port", str(port)])
    
    logger.info(f"Starting Pipecat with args: {sys.argv[1:]}")
    
    # Run Pipecat
    from pipecat.runner.run import main
    main()